%\documentclass[a4paper,10pt]{article}
%\usepackage[utf8x]{inputenc}


\documentclass[letterpaper,12pt,onecolumn]{article}
\usepackage[margin=1in]{geometry} %one inch margins
\usepackage{amsmath,graphicx}
\usepackage{algorithmic}
\usepackage{algorithm}


\usepackage{amsmath} 
\usepackage{mathtools}
\usepackage{amssymb}
\usepackage{algorithm,algorithmic}
\usepackage{caption}

\geometry{verbose,margin=1in}

\title{Distributed PageRank - Final Report}
\author{Aaron Myers, Megan Ruthven}

\begin{document}
\maketitle
\tableofcontents
\pagebreak
\section{Introduction}
The purpose of this project is to investigate distributed PageRank by applying Pagerank methods currently in use in the multi-thread case and implement it in the distributed setting. It is to determine if these methods can be scaled to multi-machine systems. We also take a method that has already proven to be effective in the distributed setting and apply it to the Pagerank problem Alternating Direction Method of Multipliers (ADMM) \cite{ADMM}). Immediately below is a more detailed description of each approach. The primary metrics for performace will be: speedup, scalability, and ease of implementation, all of which will be explained in the results section. The last metric is included to suggest that ease of coding and understanding of a method has an impact on the adoption rate in industry and therefore should be included in this investigation.

We chose to investigate ADMM \cite{ADMM} because it would have higher performance for ease of coding and understanding. We applied ADMM to the linear form of the Pagerank problem (Ax=b), but it is more difficult to apply a data-driven approach to the separated minimization problems, so to investigate data-driven approaches outlined in~\cite{Joyce}, we implemented data-driven power iteration with MPI and openMP. Typical power iteration will be refered to as Topology driven. The data-driven methods were Pull, Pull-Push, and Push based. The details of each permutation of the data-driven methods will be discussed in following sections. Because each method is implemented on distributed nodes on a big dataset, each method took advantage of the separate memories, and divided the connection matrices between nodes. There, we implemented a static load balancing regime based on amount of nonzeros in the assigned rows, which proved to be more efficient than load balancing on the number of x elements to calculate. Load balancing will be explained more in the following sections. 

The methods' performance will be compared on total time, speed up, and scalability over varying amount of nodes (MPI), and cores per node (openMP). And finally, we will go over the limitations of the methods in MPI and openMP, and future work. 

\section{Related Work}

\begin{enumerate}
  \item \cite{Fast Parallel} - This paper focuses on iterative solvers for the formulation Ax=b using well known methods including but not limited to: GMRES, BiCGSTAB, CGS, Chebychev iterations, along with Jacobi iteration methods. Their results indicated that normal Power Iteration and Jacobi methods almost always performed better than the other linear solvers and although we do not have access to their data sets nor machines, we will attempt to generally compare our speedup curves to theirs and expect some correlation. We will also expect that our power iteration and data-driven approaches will perform better than our ADMM solver for Ax=b as they did for \cite{Fast Parallel}.
  \item \cite{MPIPR} - The method descibed in this paper, used a weighted graph so we would expect the implementation to be slightly different. However, the primary take=away from this paper is that we should expect the compute time to decrease as the MPI processes increases, up to a certain point. As seen in the figure presented in the paper, with a 1M node graph, after about 30 cores, the solve time starting to trend slightly upward. We assume this increase in solve time is primarily due to the increase in message passing overhead. This would suggest that we should see similar results and also that there may be an optimal core number depending on the size of the data. 
  \item \cite{Joyce} - We have worked extremely closely with Joyce Whang as many of our algorithms were taken directly from her research paper and other members of her research group. Aside of our ADMM solver, our paper focuses on implementing her ideas in a distributed setting (as her implementation focuses only on the multi-thread case) either to confirm or deny the idea that her algorithms could also be effective in a distributed setting.
\end{enumerate}


\section{Algorithms and Implementation}

This section discusses the implementations of different methods for computing pagerank. This includes power iteration, ADMM, and data-driven approach. 

\subsection{Topology-driven Power Iteration}
This is the classic implmentation for calculating Pagerank, seen in algorithm~\ref{alg:top} . It is straight forward, but recalculates all X values every iteration. This could be redundant on a large percentage of the X indices, and therefore, it could waste computational power. 

\begin{algorithm}
\caption{Topology-driven Pagerank}
\label{alg:top}
\begin{algorithmic}[1]
  \STATE Input: graph $P_{r} = (V_r, E_r)$, $\alpha$, $\epsilon$
  \STATE Output: Pagerank $\mathbf{x}$
  \STATE Initialize $\mathbf{x} = \alpha \mathbf{e}$
  \STATE $\exists  \medspace \mathbf{x_r}$ in $\mathbf{x}$ as $P_r$ is to $P$
  \WHILE{true}
	\FOR{$v \in V_r$}
		\STATE $x_{v}^{(k+1)} = \alpha + (1 - \alpha) \sum_{w \in S_v} \frac{x_{w}^{(k)}}{|T_w|} $
		\STATE $\delta_{v} = | x_{v}^{(k+1)} - x_{v}^{(k)} | $
	\ENDFOR
	\STATE sync all $\mathbf{x_r}$ between nodes
	\IF{$\|\delta \|_{\infty} < \epsilon$}
		\STATE break;
	\ENDIF
  \ENDWHILE
  \STATE $\mathbf{x} = \frac{\mathbf{x} }{\|x\|_{1}}$
\end{algorithmic}
\end{algorithm}

In the topological algorithm, every $x_v$ is updated. Each node computes a subsection of $\mathbf{x}$, named $\mathbf{x_r}$, then all of the nodes combine their new result in batch to create a new $\mathbf{x}$ in order to continue to compute the newer values of $\mathbf{x_r}$. All of the recomputation within each node only updates the current $x_v$, this creates for a thread safe algorithm.

\subsection{Data-Driven Pagerank}
In addition to the ADMM implementation, we implemented power iteration, as well as three permutations of the data-driven pagerank method (pull, pull-push, and push). The data-driven method (taken from \cite{Joyce}) aims to minimize unnecessary computation by only updating pageranks of elements whose incoming connections were upated to a satisfactory degree. 

\begin{algorithm}
\caption{Pull Data-driven Pagerank}
\label{alg:pull}
\begin{algorithmic}[1]
  \STATE Input: graph $P_{c} = (V_c, E_c)$, $\alpha$, $\epsilon$
  \STATE Output: Pagerank $\mathbf{x}$
  \STATE Initialize $\mathbf{x} = \alpha \mathbf{e}$ and $\mathbf{t_c} = true \times \mathbf{e_c}$
  \STATE $\exists  \medspace \mathbf{x_c}$ in $\mathbf{x}$ as $P_c$ is to $P$
  \WHILE{$\exists \medspace v \medspace s.t. \medspace t_v = true$}
	\STATE $t^{new} = false \times \mathbf{e}$
	\FOR{$v \in V_c$}
		\IF{$t_v = true$}
			\STATE $x_{v}^{new} = \alpha + (1 - \alpha) \sum_{w \in S_v} \frac{x_{w}}{|T_w|} $
			\IF{ $| x_{v}^{new} - x_{v} | \geq \epsilon  $}
				\STATE $x_v = x_v^{new}$
				\FOR{$w \in T_v$}
					\STATE $t_w^{new} = true$
				\ENDFOR
			\ENDIF
		\ENDIF
	\ENDFOR
	\STATE sync all $\mathbf{x_c}$ between nodes
	\STATE logical or all $t = t^{new}$ between nodes
  \ENDWHILE
  \STATE $\mathbf{x} = \frac{\mathbf{x} }{\|x\|_{1}}$
\end{algorithmic}
\end{algorithm}


The Pull data-driven method in algorithm~\ref{alg:pull} selectively updates the values of $x_v$ based on if an incoming connected node updated itself above the threshold, $\epsilon$. This is determined by the array $t_c$, which is a set of $c$ indices for that node. In the computation of $x_v$, if the difference between the new and old value is above $\epsilon$, set all outgoing nodes' $t_w$ to true. Notice that each element, $v$, updates its value, $x_v$, and indices of $t$ both in and out of set $c$. This requires the syncing of all copies of $t^{new}$ between nodes with a logical or. Additionally, this algorithm accesses more memory by accessing incoming nodes' pagerank and outgoing nodes' $t^{new}$.

\begin{algorithm}
\caption{Pull-Push Data-driven Pagerank}
\label{alg:pullpush}
\begin{algorithmic}[1]
  \STATE Input: graph $P_{c} = (V_c, E_c)$, $\alpha$, $\epsilon$
  \STATE Output: Pagerank $\mathbf{x}$
  \STATE Initialize $\mathbf{x} = \alpha \mathbf{e}$
  \STATE $\exists  \medspace \mathbf{x_c}$ in $\mathbf{x}$ as $P_c$ is to $P$
  \STATE Initialize $\mathbf{r} = \mathbf{0}$
  \FOR{$v \in V_c$}
	\FOR{$w \in S_v$}
		\STATE $r_v = r_v + \frac{1}{|T_w|}$
	\ENDFOR
	\STATE $r_v = (1 - \alpha)\alpha r_v$
  \ENDFOR
  \WHILE{$\exists \medspace v \medspace s.t. \medspace r_v \geq \epsilon$}
	\STATE $r^{new} = \mathbf{0}$
	\FOR{$v \in V_c$}
		\IF{$r_v \geq \epsilon$}
			\STATE $x_{v} = \alpha + (1 - \alpha) \sum_{w \in S_v} \frac{x_{w}}{|T_w|} $
			\FOR{$w \in T_v$}
				\STATE $r_w^{new} = r_w^{new} + \frac{r_v \alpha}{|T_v|}$
			\ENDFOR
		\ELSE
			\STATE $r_v^{new} = r_v^{new} + r_v$
		\ENDIF
	\ENDFOR
	\STATE sync all $\mathbf{x_c}$ between nodes
	\STATE add all and scatter $ r_c = r^{new}$ between nodes
  \ENDWHILE
  \STATE $\mathbf{x} = \frac{\mathbf{x} }{\|x\|_{1}}$
\end{algorithmic}
\end{algorithm}

The Pull-Push data-driven method in algorithm~\ref{alg:pullpush} selectively updates the values of $x_v$ based on if its residual is above a threshold, $\epsilon$. The residuals are accounted for in the $\mathbf{r}$ vector and batch updated to transfer residual values between nodes. Notice that each element, $v$, updates its value, $x_v$, and indices of $r$ both in and out of set $c$. This requires the syncing of all copies of $r^{new}$ between nodes by summing. Additionally, this algorithm accesses more memory by accessing incoming nodes' pagerank and outgoing nodes' $r^{new}$.

\begin{algorithm}
\caption{Push Data-driven Pagerank}
\label{alg:push}
\begin{algorithmic}[1]
  \STATE Input: graph $P_{c} = (V_c, E_c)$, $\alpha$, $\epsilon$
  \STATE Output: Pagerank $\mathbf{x}$
  \STATE Initialize $\mathbf{x} = \alpha \mathbf{e}$
  \STATE $\exists  \medspace \mathbf{x_c}$ in $\mathbf{x}$ as $P_c$ is to $P$
  \STATE Initialize $\mathbf{r} = \mathbf{0}$
  \FOR{$v \in V_c$}
	\FOR{$w \in S_v$}
		\STATE $r_v = r_v + \frac{1}{|T_w|}$
	\ENDFOR
	\STATE $r_v = (1 - \alpha)\alpha r_v$
  \ENDFOR
  \WHILE{$\exists \medspace v \medspace s.t. \medspace r_v \geq \epsilon$}
	\STATE $r^{new} = \mathbf{0}$
	\FOR{$v \in V_c$}
		\IF{$r_v \geq \epsilon$}
			\STATE $x_{v} = x_{v} + r_v $
			\FOR{$w \in T_v$}
				\STATE $r_w^{new} = r_w^{new} + \frac{r_v \alpha}{|T_v|}$
			\ENDFOR
		\ELSE
			\STATE $r_v^{new} = r_v^{new} + r_v$
		\ENDIF
	\ENDFOR
	\STATE add all and scatter $ r_c = r^{new}$ between nodes
  \ENDWHILE
\STATE sync all $\mathbf{x_c}$ between nodes
  \STATE $\mathbf{x} = \frac{\mathbf{x} }{\|x\|_{1}}$
\end{algorithmic}
\end{algorithm}

The Push data-driven method in algorithm~\ref{alg:push} selectively updates the values of $x_v$ based on if its residual is above a threshold, $\epsilon$. It updates $x_v$ by only using $r_v$. This accesses less memory by bypassing fetching all of the incoming nodes current values $x_w$. The residuals are accounted for in the $\mathbf{r}$ vector and batch updated to transfer residual values between nodes. Notice that each element, $v$, updates its value, $x_v$, and indices of $r$ both in and out of set $c$. This requires the syncing of all copies of $r^{new}$ between nodes by summing. 

Although MPI allowed for a distributed Pagerank algorithm, there were a couple of restrictions on the implementation because of the MPI and openMP framework we chose to use. When syncing values between nodes, MPI requires all nodes to simultaneously request and send the information they want. This means that all of the independent nodes have to update the values in batch. This is was not ideal because it meant that partial residuals were silowed in different nodes. The distributedness of each residual disallows for a priority queue and asynchronis updating. 

NOTE: THIS NEEDS MORE INFORMATION 

\subsection{Linear System Approach}
This approach requires that we form the Pagerank problem into a different linear system (Ax=b) where we fundamentally look at methods of iterating or directly solving for an inverse to solve for x which would would provide the list of Pagerank values. Below is a simple derivation taken from \cite{Fast Parallel}.
\begin{center}
\begin{align}
	P' &= P + dv^{T} \\
	P'' &= cP' + (1-c)ev^{T} \\
	x^{k+1} &= P''^{T}x^{k}
\end{align}

\end{center}
Where P$'$ and P$''$ are the modified PageRank matrices that have the modifications necessary to create a connected graph and add a personalization factor and e is a vector of all 1's, resulting in equation 3, the Power Iteration approach to Pagerank.
\newline
\linebreak
Given the additional information below, we can derive the linear system for Pagerank.


\begin{center}
\begin{align}
  e^{T}x & = x^{T}e = \|x\|_{1} = \|x\| \\
  d^{T}x &= \| x\| - \| P^{T}x\| \\
  x &= [cP^{T} + c(vd^{T}) + (1-c)ve^{T}]x
\end{align}
\end{center}

Combining the information above, we arrive at the following equation:

\begin{center}
\begin{equation}
  (I-cP^{T})x = kv
\end{equation}
\end{center}
We now have Pagerank in a linear form (Ax=b), where A = I-c$P^{T}$ and kv = b. If in addition, we normalize x, we have the following:

\begin{center}
  \begin{align}
	k &= \|x\| - c \|P^{T}x\| = (1-c) \|x\| + d^{T}x \\	
	k &= 1-c 
  \end{align}
\end{center}



\subsection{ADMM}

Many of the articles we encountered for solving parallel pagerank in the linear form used Jacobi iteration or some Krylov Subspace method (GMRES, BiCGSTAB, etc), but we attempted to implement something we were introduce to in this course, namely ADMM \cite{ADMM}. This is an extremely simple way to parallelize a linear solve. This process attempts to split the linear problem into subsections, solve separately, and combine the information in a very specific way. 
We will compare these results to the implementation of GMRES (Generalized Minimal RESidual method) and BiCGSTAB (BiConjugate Gradient method with STABilization) for the same problem parallelizing using PETSc (a software with great tools for parallizing linear solvers) \cite{Power Law Graphs}. We expect ADMM to have worse performance, measured by speedup, but we would like to quantify the loss in accuracy/time relative to the ease of implementation and scalability.
\newline
\linebreak
Below is a brief description of the ADMM idea and algorithm \cite{ADMM}.
\newline
We take the linear problem and split up the data accordingly:
\begin{center}
\begin{align}
	A &= \left[ A_{1} ... A_{n} \right]' \\
	b &= \left[ b_{1} ... b_{n} \right]' 
\end{align}
\end{center}
Our origninal minimization of Ax-b with a certain norm and regulariztion on x now becomes:

\begin{center}
\begin{align}
	&minimize \: \: \: \sum_{i=1}^{N} l_{i}(A_{i}x_{i} - b_{i}) + r(z) \\
	&subject \: \: \: to \: \: \: x_{i} - z = 0 \: \: \: \forall i
\end{align}
\end{center}
Where $x_{i}$ are local variables that we force to match the global solution z at each step and N is the number of processes used to solve the problem. This method also includes an 'augmented lagragian' term. This term is used to bring robustness to the dual ascent problem and result in convergence without criteria like strict convexity or finiteness of the function. This is discussed in much greater detail in \cite{ADMM}. Below is the augemented lagrangian which is used to derive the resulting algorithm.
\newline
\begin{center}
  \begin{align}
		L_{\rho}(x,y) &= f(x) + y^{T}(Ax-b) + \frac{\rho}{2}\|Ax - b\|_{2}^{2} \\
  \end{align}
\end{center}


The resulting algorithm, using the augmented lagrangian presented in the ADMM method \cite{ADMM}, is as follows:

\begin{center}
\begin{algorithm}
\caption{ADMM Iteration}
\begin{algorithmic}[1]
	\STATE $x_{i}^{k+1} = argmin_{x} \: \: \: l_{i}(A_{i}x_{i} - b_{i}) + \frac{\rho}{2} \| x_{i}^{k} - z^{k} - u_{i}^{k} \|_{2}^{2}$ 
	\STATE $z^{k+1} = argmin_{z} \: \: \: r(z) + \frac{N \rho}{x} \| z^{k} - \bar{x}^{k+1} - \bar{u}^{k} \|_{2}^{2} $
	\STATE $u_{i}^{k+1} = u_{i}^{k} + x_{i}^{k+1} - z^{k+1} $ 
  \end{algorithmic}
\end{algorithm}
\end{center}

Where $u_{i}^{k} = \frac{1}{\rho} y_{i}^{k}$ and for our implementation, we chose the $L^{1}$ regularization term (also known as lasso) and used a gradient solver with Eigen to solve the minimization for x. Using the lasso regulariztion the z update becomes soft-thresholding update. Considering the Lasso method, the updated algorithm is below.

\begin{center}
\begin{algorithm}
\caption{ADMM Iteration with Lasso}
\begin{algorithmic}[1]
  \STATE $x_{i}^{k+1} = argmin_{x} \: \: \: \|(A_{i}x_{i} - b_{i})\|_{2}^{2} + \frac{\rho}{2} \| x_{i}^{k} - z^{k} - u_{i}^{k} \|_{2}^{2}$ 
  \STATE $z^{k+1} = S_{\lambda/\rho N} (\bar{x}^{k+1} - \bar{u}^{k})$
	\STATE $u_{i}^{k+1} = u_{i}^{k} + x_{i}^{k+1} - z^{k+1} $ 
  \end{algorithmic}
\end{algorithm}
\end{center}

Where S is defined componend-wise in the following way:
\begin{center}
	\begin{equation}
	  S_{\lambda/\rho N}(x_{i}) := (x_{i} - \frac{\lambda}{\rho N})_{+} - (-x_{i} - \frac{\lambda}{\rho N})_{+}
	\end{equation}
\end{center}

\section{Load Balancing}
First we implemented our methods where each node had the same amount of X's to compute. To allow for more equality in the amount of computations each node had to do, our implementations moved to a method where each node were assigned a continuous set of indices where each node had the same amount of nonzero connections between the indices and other nodes. 


\section{Results}
The different methods had varying quantitative and qualitative results. These included the time to complete, speedup, scalability, and ease of coding. 

\subsection{Power Iteration Results}
This is where graphs would go 


\subsection{ADMM Results Compared to other Linear methods}
Below are the inital results for ADMM programmed in Matlab for a simple data set (a disconnected synthetic 11 node graph). 

\begin{center}
\captionof{table}{Linear Method Table}
\begin{tabular}{l || r}
	\hline\hline
	\multicolumn{2}{c}{5 iter results} \\
	\hline\hline
	Method  & $\|\hat{x} - x\| $ \\
	\hline
	GMRES & 0.0047  \\
	BiCGSTAB & 0.0056  \\
	ADMM & 0.0079  \\
\end{tabular}
\end{center}


\section{Future Work}

Now that we have seen the value of both ADMM and data-driven PageRank, the next steps would be the following.

\begin{enumerate}
  \item Optimize our MPI approach with the data-driven methods in an attempt to get the results to match the multi-thread implementation of \cite{Joyce}.
  \item Take the results of this investiation as an indicator that expanding the galois code from \cite{Joyce} to a distributed setting might be valuable.
  \item Apply ADMM to with other minimization solver as an attempt to find the optimal solver.
\end{enumerate}


\section{Conclusion}

This bib actually needs to be in a separate bib file
\begin{thebibliography}{1}

\bibitem{Power Law Graphs} David Gleich, et al. {\em Scalable Computing for Power Law Graphs: Experience with Parallel PageRank}

\bibitem{Fast Parallel} David Gleich, et al. {\em Fast Parallel PageRank: A Linear System Approach} 

\bibitem{ADMM} Stephen Boyd, et al. {\em Distributed Optimization and Statistical Learning via the Alternating Direction Method of Multipliers} 

\bibitem{Joyce} Joyce Whang, et al. {\em Scalable Data-driven PageRank: Algorithms, System Issues '\&' Lessons Learned}
 
 \bibitem{MPIref} mcs.anl.gov {\em http://www.mcs.anl.gov/research/projects/mpi/tutorial/mpiexmpl/src/jacobicmpl/C/main.html}
 
 \bibitem{FSU} fsu.edu {\em http://people.sc.fsu.edu/~jburkardt/cppsrc/mpi/mpi.html}

 \bibitem{MPIPR} Xiaoyi (Eric) Li  {\em Parallel PageRank Computation using MPI}

 \bibitem{YahooPPR} David Gleich, Leonid Zhukov, Pavel Berkhin  {\em Fast Parallel PageRank: Methods and Evaluations}

 \bibitem{IndianaPPR} Shubhada Karavinkoppa and Jayesh Kawli  {\em Page Rank Algorithm Using MPI}

 \bibitem{MixedMPI} Bundit Manaskasemsak, Putchong Uthayopas, Arnon Rungsawang {\em A Mixed MPI-Thread Approach for Parallel Page Ranking Computation}

\end{thebibliography}

\end{document}
